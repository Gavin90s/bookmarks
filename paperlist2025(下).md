#### [Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data](https://assets.amazon.science/79/5d/e64b592c479a85d046cda12a2bb9/learning-with-less-knowledge-distillation-from-large-language-models-via-unlabeled-data.pdf)
![image](https://github.com/user-attachments/assets/a80e2902-cccd-4d84-acfd-5f39d01dfce1)
````
Learning with Less computational resources and less data for Knowledge Distillation from LLMs.
LLKD 使用自适应样本选择(adaptive sample selection)方法, 挑选teacher认为置信度高(high confidence)，
但student判断到不确定性高(high uncertainty)的样本。
````

#### [DIRECT REASONING OPTIMIZATION: LLMS CAN REWARD AND REFINE THEIR OWN REASONING FOR OPEN-ENDED TASKS](https://arxiv.org/pdf/2506.13351)

#### [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://www.arxiv.org/pdf/2506.03143)

#### [Keeping Humans in the Loop:Human-Centered Automated Annotation with Generative AI](https://arxiv.org/pdf/2409.09467)

#### [LLMS GET LOST IN MULTI-TURN CONVERSATION](https://arxiv.org/pdf/2505.06120)

#### [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/pdf/2310.11441)

#### [IMPROVE VISION LANGUAGE MODEL CHAIN-OFTHOUGHT REASONING](https://arxiv.org/pdf/2410.16198)

#### [Group Sequence Policy Optimization](https://arxiv.org/pdf/2507.18071)

#### [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)

#### [Qwen2.5-Omni Technical Report](https://arxiv.org/pdf/2503.20215)

#### [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](https://arxiv.org/pdf/2407.18961)
https://github.com/apple/axlearn/tree/main/docs/research/mmau

#### [GUI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/pdf/2501.12326)

#### [AppAgentX: Evolving GUI Agents as Proficient Smartphone Users](https://arxiv.org/pdf/2503.02268)
https://github.com/Westlake-AGI-Lab/AppAgentX/blob/main/explor_auto.py

#### [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/pdf/2507.14447)

#### [KIMI K2: OPEN AGENTIC INTELLIGENCE](file:///Users/bytedance/Downloads/tech_report%20.pdf)

#### [Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)

#### [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388)

#### [DIRECT REASONING OPTIMIZATION: LLMS CAN REWARD AND REFINE THEIR OWN REASONING FOR OPEN-ENDED TASKS](https://arxiv.org/pdf/2506.13351)

#### [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://www.arxiv.org/pdf/2506.03143)

#### [Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI](https://arxiv.org/pdf/2409.09467)

#### [LLMS GET LOST IN MULTI-TURN CONVERSATION](https://arxiv.org/pdf/2505.06120)

#### [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/pdf/2505.02881)

#### [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/pdf/2504.11257)

#### [Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://arxiv.org/pdf/2408.16725)

#### [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/pdf/2508.09736)

#### [VERIGUI: VERIFIABLE LONG-CHAIN GUI DATASET](https://arxiv.org/pdf/2508.04026)

#### [Lost in Transmission:When and Why LLMs Fail to Reason Globally](https://arxiv.org/pdf/2505.08140)

#### [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/pdf/2508.10833)

#### [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/pdf/2407.06135)

#### [MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text](https://arxiv.org/pdf/2210.02928)

#### [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)

#### [Mixed Preference Optimization: A Two-stage Reinforcement Learning with Human Feedbacks](https://arxiv.org/pdf/2403.19443)

#### [Token Level Routing Inference System for Edge Devices](https://arxiv.org/pdf/2504.07878)

#### [RL’S RAZOR: WHY ONLINE REINFORCEMENT LEARNING FORGETS LESS](https://arxiv.org/pdf/2509.04259)

#### [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/pdf/2510.01265)
````
RLP 把“让模型自己写 CoT→看是否更好预测下一词”作为自监督奖励，首次把强化学习的探索机制提前到预训练阶段，
显著且高效地为大模型注入推理能力，为“预训练即推理”提供了新范式。
````
<img width="1950" height="920" alt="image" src="https://github.com/user-attachments/assets/bb8ee8b8-f3dd-4915-9d4b-787e3b0bbe44" />

#### [JokeEval: Are the Jokes Funny? Review of Computational Evaluation Techniques to improve Joke Generation](https://assets.amazon.science/82/13/9941bff84d2eaaa3f68c7a340d88/jokeeval-are-the-jokes-funny-review-of-computational-evaluation-techniques-to-improve-joke-generation.pdf)

#### [Ambiguity Detection and Uncertainty Calibration for Question Answering with Large Language Models](https://assets.amazon.science/e8/7b/9de54e5442f9a4dd354f7e26f290/ambiguity-detection-and-uncertainty-calibration-for-question-answering-with-large-language-models.pdf)

#### [TopoSem: In-Context Planning with Semantically-Informed Tooling Graph Similarity](https://assets.amazon.science/98/f2/02462d7e409fbc2ed777f6a7c95a/toposem-in-context-planning-with-semantically-informed-tooling-graph-similarity.pdf)
````
TopoSem框架：提出TopoSem框架，通过联合考虑API执行图的拓扑距离和API负载的语义差异来增强ICP。利用对比学习方法学习有意义的嵌入，
再用于增强的动态聚类机制，以减少示例选择中的噪声和冗余。
图编辑距离（GED）：定义了一种新的距离度量，基于API执行图的GED，其中节点和边操作根据相应API负载的语义相似性进行加权。
节点替换成本考虑了API负载的语义差异和API本身是否相同两个因素。
数据增强：提出一种合成数据增强管道，扩充现有的（查询，API图）对，以更好地调整语义嵌入。通过四种策略生成新的查询和API图对，
以增加训练数据的多样性和覆盖度。
动态聚类：使用凝聚聚类对检索到的Top-K示例进行聚类，从每个聚类中选择一个示例形成最终的示例集，以增加示例的多样性并减少冗余。
````

#### [A Systematic Survey of Automatic Prompt Optimization Techniques](https://arxiv.org/pdf/2502.16923)
<img width="2278" height="554" alt="image" src="https://github.com/user-attachments/assets/09759cc0-a1c6-445f-a597-34c64de23dd6" />

#### [FaVe: Factored and Verified Search Rationale for Long-form Answer](https://assets.amazon.science/ab/ec/c27218534e17b398677b12903dc1/scipub-approval152129-36873472-fave-factored-and-verified-search-rationale-for-longform-answer.pdf)

#### [CUDA 编程指南](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
<img width="1212" height="1588" alt="image" src="https://github.com/user-attachments/assets/14735856-7ec7-4466-acbe-8dd76bea0f65" />

#### [Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning](https://arxiv.org/pdf/2502.08972)
````
TICL方法
核心思想：TICL通过迭代地扩展上下文学习提示（in-context learning prompt），引入模型生成的负样本和对这些负样本的解释，
从而在不更新模型参数的情况下，使模型更好地理解和模仿特定用户的写作风格。
主要步骤：
行为克隆：使用少量用户文本作为示例，通过上下文学习来引导模型生成符合用户风格的文本。
探索：模型生成文本，然后分析这些文本与用户实际文本之间的风格差异，生成负样本。
解释与学习：为负样本生成解释，说明其风格与用户文本的不同之处，并将这些负样本和解释添加到提示中，以便在后续迭代中避免生成类似风格的文本。
优势：TICL无需对模型进行微调，仅通过增加推理计算量来实现个性化，适用于数据有限且对隐私有要求的场景。
````
