#### [Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data](https://assets.amazon.science/79/5d/e64b592c479a85d046cda12a2bb9/learning-with-less-knowledge-distillation-from-large-language-models-via-unlabeled-data.pdf)
![image](https://github.com/user-attachments/assets/a80e2902-cccd-4d84-acfd-5f39d01dfce1)
````
Learning with Less computational resources and less data for Knowledge Distillation from LLMs.
LLKD 使用自适应样本选择(adaptive sample selection)方法, 挑选teacher认为置信度高(high confidence)，
但student判断到不确定性高(high uncertainty)的样本。
````

#### [DIRECT REASONING OPTIMIZATION: LLMS CAN REWARD AND REFINE THEIR OWN REASONING FOR OPEN-ENDED TASKS](https://arxiv.org/pdf/2506.13351)

#### [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://www.arxiv.org/pdf/2506.03143)

#### [Keeping Humans in the Loop:Human-Centered Automated Annotation with Generative AI](https://arxiv.org/pdf/2409.09467)

#### [LLMS GET LOST IN MULTI-TURN CONVERSATION](https://arxiv.org/pdf/2505.06120)

#### [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/pdf/2310.11441)

#### [IMPROVE VISION LANGUAGE MODEL CHAIN-OFTHOUGHT REASONING](https://arxiv.org/pdf/2410.16198)

#### [Group Sequence Policy Optimization](https://arxiv.org/pdf/2507.18071)

#### [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)

#### [Qwen2.5-Omni Technical Report](https://arxiv.org/pdf/2503.20215)

#### [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](https://arxiv.org/pdf/2407.18961)
https://github.com/apple/axlearn/tree/main/docs/research/mmau

#### [GUI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/pdf/2501.12326)

#### [AppAgentX: Evolving GUI Agents as Proficient Smartphone Users](https://arxiv.org/pdf/2503.02268)
https://github.com/Westlake-AGI-Lab/AppAgentX/blob/main/explor_auto.py

#### [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/pdf/2507.14447)

#### [KIMI K2: OPEN AGENTIC INTELLIGENCE](file:///Users/bytedance/Downloads/tech_report%20.pdf)

#### [Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/pdf/2501.11873)

#### [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388)

#### [DIRECT REASONING OPTIMIZATION: LLMS CAN REWARD AND REFINE THEIR OWN REASONING FOR OPEN-ENDED TASKS](https://arxiv.org/pdf/2506.13351)

#### [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://www.arxiv.org/pdf/2506.03143)

#### [Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI](https://arxiv.org/pdf/2409.09467)

#### [LLMS GET LOST IN MULTI-TURN CONVERSATION](https://arxiv.org/pdf/2505.06120)

#### [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/pdf/2505.02881)

#### [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/pdf/2504.11257)

#### [Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://arxiv.org/pdf/2408.16725)

#### [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/pdf/2508.09736)

#### [VERIGUI: VERIFIABLE LONG-CHAIN GUI DATASET](https://arxiv.org/pdf/2508.04026)

#### [Lost in Transmission:When and Why LLMs Fail to Reason Globally](https://arxiv.org/pdf/2505.08140)

#### [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/pdf/2508.10833)

#### [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/pdf/2407.06135)

#### [MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text](https://arxiv.org/pdf/2210.02928)

#### [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)

#### [Mixed Preference Optimization: A Two-stage Reinforcement Learning with Human Feedbacks](https://arxiv.org/pdf/2403.19443)

#### [Token Level Routing Inference System for Edge Devices](https://arxiv.org/pdf/2504.07878)

#### [RL’S RAZOR: WHY ONLINE REINFORCEMENT LEARNING FORGETS LESS](https://arxiv.org/pdf/2509.04259)

#### [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/pdf/2510.01265)
````
RLP 把“让模型自己写 CoT→看是否更好预测下一词”作为自监督奖励，首次把强化学习的探索机制提前到预训练阶段，
显著且高效地为大模型注入推理能力，为“预训练即推理”提供了新范式。
````
<img width="1950" height="920" alt="image" src="https://github.com/user-attachments/assets/bb8ee8b8-f3dd-4915-9d4b-787e3b0bbe44" />

#### [JokeEval: Are the Jokes Funny? Review of Computational Evaluation Techniques to improve Joke Generation](https://assets.amazon.science/82/13/9941bff84d2eaaa3f68c7a340d88/jokeeval-are-the-jokes-funny-review-of-computational-evaluation-techniques-to-improve-joke-generation.pdf)

#### [Ambiguity Detection and Uncertainty Calibration for Question Answering with Large Language Models](https://assets.amazon.science/e8/7b/9de54e5442f9a4dd354f7e26f290/ambiguity-detection-and-uncertainty-calibration-for-question-answering-with-large-language-models.pdf)

#### [TopoSem: In-Context Planning with Semantically-Informed Tooling Graph Similarity](https://assets.amazon.science/98/f2/02462d7e409fbc2ed777f6a7c95a/toposem-in-context-planning-with-semantically-informed-tooling-graph-similarity.pdf)
````
TopoSem框架：提出TopoSem框架，通过联合考虑API执行图的拓扑距离和API负载的语义差异来增强ICP。利用对比学习方法学习有意义的嵌入，
再用于增强的动态聚类机制，以减少示例选择中的噪声和冗余。
图编辑距离（GED）：定义了一种新的距离度量，基于API执行图的GED，其中节点和边操作根据相应API负载的语义相似性进行加权。
节点替换成本考虑了API负载的语义差异和API本身是否相同两个因素。
数据增强：提出一种合成数据增强管道，扩充现有的（查询，API图）对，以更好地调整语义嵌入。通过四种策略生成新的查询和API图对，
以增加训练数据的多样性和覆盖度。
动态聚类：使用凝聚聚类对检索到的Top-K示例进行聚类，从每个聚类中选择一个示例形成最终的示例集，以增加示例的多样性并减少冗余。
````

#### [A Systematic Survey of Automatic Prompt Optimization Techniques](https://arxiv.org/pdf/2502.16923)
<img width="2278" height="554" alt="image" src="https://github.com/user-attachments/assets/09759cc0-a1c6-445f-a597-34c64de23dd6" />

#### [FaVe: Factored and Verified Search Rationale for Long-form Answer](https://assets.amazon.science/ab/ec/c27218534e17b398677b12903dc1/scipub-approval152129-36873472-fave-factored-and-verified-search-rationale-for-longform-answer.pdf)
<img width="1212" height="1588" alt="image" src="https://github.com/user-attachments/assets/14735856-7ec7-4466-acbe-8dd76bea0f65" />

#### [CUDA 编程指南](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)

#### [Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning](https://arxiv.org/pdf/2502.08972)
````
TICL方法
核心思想：TICL通过迭代地扩展上下文学习提示（in-context learning prompt），引入模型生成的负样本和对这些负样本的解释，
从而在不更新模型参数的情况下，使模型更好地理解和模仿特定用户的写作风格。
主要步骤：
行为克隆：使用少量用户文本作为示例，通过上下文学习来引导模型生成符合用户风格的文本。
探索：模型生成文本，然后分析这些文本与用户实际文本之间的风格差异，生成负样本。
解释与学习：为负样本生成解释，说明其风格与用户文本的不同之处，并将这些负样本和解释添加到提示中，以便在后续迭代中避免生成类似风格的文本。
优势：TICL无需对模型进行微调，仅通过增加推理计算量来实现个性化，适用于数据有限且对隐私有要求的场景。
````

#### [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://assets.amazon.science/16/67/383a42e343e79cdaabb5dacc84ea/3937-group-aware-reinforcement-camera-ready.pdf)

#### [Open Domain Question Answering with Conflicting Contexts](https://assets.amazon.science/0c/a1/3066c5854e8e943ecd14d42d8f15/open-domain-question-answering-with-conflicting-contexts.pdf)
<img width="2310" height="998" alt="image" src="https://github.com/user-attachments/assets/1744d60b-9c43-4db3-ac8b-ce2aa8d1455e" />

#### [Trustworthiness-as-Reward: Improving LLM Performance on Text Classification through Reinforcement Learning](https://assets.amazon.science/ea/88/a818c83e4daaa3d920d911747d97/scipub-approval152129-40236693-trustworthinessasreward-improving-llm-performance-on-text-classification-through-reinforcement-learning.pdf)

#### [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://assets.amazon.science/b5/2e/9db326c7421b967fe0903d6307b9/think-clearly-improving-reasoning-via-redundant-token-pruning.pdf)

#### [Building Multi-Turn RAG for Customer Support with LLM Labeling](https://assets.amazon.science/30/1b/6aca1b504a588cc204adbe49d34f/building-multi-turn-rag-for-customer-support-with-llm-labeling.pdf)
````
提出 Multi-Turn-RAG pipeline：
用 现有客服日志（无标注）+ 知识库文章，让 LLM 扮演“用户+客服”自我对话→自动合成多轮对话；
同时让 LLM 输出每轮 是否需要检索、检索查询、支撑文档、最终回复 等 4 类标签，全程无人工。
设计 “检索决策”奖励函数：
在 RL 阶段把 检索准确率、冗余度、对话轮数 量化成奖励，防止模型“偷懒”或“过度检索”。
实验结果（真实客服 2 千段人工测试）：
答案准确率 +18%（62→80）
平均对话轮数 −1.3 轮
人工满意度 +15%
训练数据仅 3 天生成，成本≈人工标注的 4%。
````
#### [Mamba Drafters for Speculative Decoding](https://assets.amazon.science/a2/d5/422de2e746e49e3afc26b8597c5f/mamba-drafters-for-speculative-decoding.pdf)

#### [ConFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions](https://github.com/amazon-science/confetti/tree/main)
````
These conversations explicitly target various conversational complexities, such as follow-ups,
goal correction and switching, ambiguous and implicit goals.
Below is a list of all included complexities and the number of dialogs covering those complexities.
````
<img width="1690" height="1360" alt="image" src="https://github.com/user-attachments/assets/8d0b64cf-2586-43d5-89c1-71447e4a6e05" />

#### [Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models](https://assets.amazon.science/3a/4a/2c2c65174dc58626cb35522aadb4/expansion-span-combining-fading-memory-and-retrieval-in-hybrid-state-space-models.pdf)

#### [REIC: RAG-enhanced intent classification at scale](https://www.amazon.science/publications/reic-rag-enhanced-intent-classification-at-scale)

#### [GEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM REINFORCEMENT LEARNING](https://arxiv.org/pdf/2507.19457)
<img width="2910" height="1746" alt="image" src="https://github.com/user-attachments/assets/0942715d-67ad-4e32-8db1-da95bf150a39" />

#### [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://www.arxiv.org/abs/2510.04618)
