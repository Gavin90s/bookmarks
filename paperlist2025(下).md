#### [Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data](https://assets.amazon.science/79/5d/e64b592c479a85d046cda12a2bb9/learning-with-less-knowledge-distillation-from-large-language-models-via-unlabeled-data.pdf)
![image](https://github.com/user-attachments/assets/a80e2902-cccd-4d84-acfd-5f39d01dfce1)
````
Learning with Less computational resources and less data for Knowledge Distillation from LLMs.
LLKD 使用自适应样本选择(adaptive sample selection)方法, 挑选teacher认为置信度高(high confidence)，
但student判断到不确定性高(high uncertainty)的样本。
````

#### [DIRECT REASONING OPTIMIZATION: LLMS CAN REWARD AND REFINE THEIR OWN REASONING FOR OPEN-ENDED TASKS](https://arxiv.org/pdf/2506.13351)

#### [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://www.arxiv.org/pdf/2506.03143)

#### [Keeping Humans in the Loop:Human-Centered Automated Annotation with Generative AI](https://arxiv.org/pdf/2409.09467)
