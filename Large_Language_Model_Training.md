#### RMS LayerNorm
&emsp;&emsp;Normalizationä¸€èˆ¬éƒ½åŒ…å«äº†å‡å‡å€¼ï¼ˆcenterï¼‰å’Œé™¤ä»¥æ ‡å‡†å·®ï¼ˆscaleï¼‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä½†è¿‘æ¥çš„ä¸€äº›å·¥ä½œé€æ¸å°è¯•å»æ‰centerè¿™ä¸€æ­¥ï¼Œç”šè‡³æœ‰äº›å·¥ä½œçš„ç»“æœæ˜¾ç¤ºå»æ‰centerè¿™ä¸€æ­¥åæ€§èƒ½è¿˜ç•¥æœ‰æå‡ã€‚<br/>
&emsp;&emsp;<img width="347" alt="image" src="https://user-images.githubusercontent.com/8350994/229290093-69f6619c-88ab-4630-bec9-1cab35f0389a.png">
<br/>&emsp;&emsp;2019å¹´çš„è®ºæ–‡[ã€ŠRoot Mean Square Layer Normalizationã€‹](https://arxiv.org/abs/1910.07467) æ¯”è¾ƒäº†å»æ‰centeråçš„Layer Normalizationï¼Œæ–‡ç« ç§°ä¹‹ä¸ºRMS Normã€‚RMS Normä¹Ÿå°±æ˜¯L2 Normalizationçš„ç®€å•å˜ä½“è€Œå·²ï¼Œä½†è¿™ç¯‡è®ºæ–‡æ€»çš„ç»“æœæ˜¾ç¤ºï¼šRMS Normæ¯”Layer Normalizationæ›´å¿«ï¼Œæ•ˆæœä¹ŸåŸºæœ¬ä¸€è‡´ã€‚
<br/>&emsp;&emsp;é™¤äº†è¿™ç¯‡æ–‡ç« å¤–ï¼ŒRMS Normè¿˜è¢«Googleç”¨åœ¨äº†T5ä¸­ï¼Œå¹¶ä¸”åœ¨å¦å¤–çš„ä¸€ç¯‡æ–‡ç«  [ã€ŠDo Transformer Modifications Transfer Across Implementations and Applications?ã€‹](https://arxiv.org/abs/2102.11972)ä¸­åšäº†æ¯”è¾ƒå……åˆ†çš„å¯¹æ¯”å®éªŒï¼Œæ˜¾ç¤ºå‡ºRMS Normçš„ä¼˜è¶Šæ€§ã€‚è¿™æ ·çœ‹æ¥ï¼Œæœªæ¥RMS Normå¾ˆå¯èƒ½å°†ä¼šå–ä»£Layer Normalizationè€Œæˆä¸ºTransformerçš„æ ‡é…ã€‚
<br/>&emsp;&emsp;ä¸€ä¸ªç›´è§‚çš„çŒœæµ‹æ˜¯ï¼Œcenteræ“ä½œï¼Œç±»ä¼¼äºå…¨è¿æ¥å±‚çš„biasé¡¹ï¼Œå‚¨å­˜åˆ°çš„æ˜¯å…³äºæ•°æ®çš„ä¸€ç§å…ˆéªŒåˆ†å¸ƒä¿¡æ¯ï¼Œè€ŒæŠŠè¿™ç§å…ˆéªŒåˆ†å¸ƒä¿¡æ¯ç›´æ¥å‚¨å­˜åœ¨æ¨¡å‹ä¸­ï¼Œåè€Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ä¸‹é™ã€‚æ‰€ä»¥T5ä¸ä»…å»æ‰äº†Layer Normalizationçš„centeræ“ä½œï¼Œå®ƒæŠŠæ¯ä¸€å±‚çš„biasé¡¹ä¹Ÿéƒ½å»æ‰äº†ã€‚

#### SwiGLU
Swishæ¿€æ´»å‡½æ•°ï¼š
<br/>&emsp;&emsp;&emsp;&emsp;ğ‘†ğ‘¤ğ‘–ğ‘ â„=ğ‘¥â‹…ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ›½ğ‘¥)
<br/>æˆ‘ä»¬ä¸éš¾å‘ç°ï¼Œæ¿€æ´»å‡½æ•°å°±æ˜¯å¯¹xä¹˜ä»¥ä¸€äº›æ•°ï¼Œä»¥å¯¹æŸäº›å€¼è¿›è¡Œçº¦æŸã€‚
<br/>GLUï¼ˆGated Linear Unitï¼‰ï¼Œå…¶ä¸€èˆ¬å½¢å¼ä¸ºï¼š
<br/>&emsp;&emsp;&emsp;&emsp;ğºğ¿ğ‘ˆ(ğ‘¥)=ğœ(ğ‘Šğ‘¥+ğ‘)âŠ—(ğ‘‰ğ‘¥+ğ‘)
<br/>![image](https://user-images.githubusercontent.com/8350994/229295009-b83833d1-b5c2-4272-ad5a-7364bd0d70dc.png)
<br/>What does the SwiGLU activation function look like?
<br/>The SwiGLU activation function is a piecewise linear function that is defined as follows:
<br/>&emsp;&emsp;&emsp;&emsp;SwiGLU(x) = max(x, 0) + min(Î±(x - ReLU(x)), 0)
<br/>where x is the input to the function, ReLU(x) is the rectified linear unit function (i.e., max(x, 0)), and Î± is a tunable parameter that controls the shape of the negative part of the function.
<br/>The SwiGLU activation function is designed to address some of the limitations of the ReLU function, which can result in "dead" neurons that do not contribute to the output of a neural network. By introducing a piecewise linear negative slope, the SwiGLU function can help to prevent this problem and improve the performance of neural networks.
